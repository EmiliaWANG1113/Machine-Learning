# Python-PySpark

## Spark
當要分析的資料大到一台電腦沒辦法處理（可能是檔案過大沒辦法載入單台電腦的記憶體、或是單台運算時間太長）的時候，通常有兩種解決方法。
1.購買一台昂貴的超級電腦(96核 CPU, 1TB Memory…) 
2.購買多台較便宜的一般電腦來分工處理原本的工作。

1. 第一種:
對於開發人員來說的好處是程式碼完全不用改直接丟上去跑。但對於企業來說為了少數工作花好幾百萬的經費購買超級電腦非常的不划算，
而且假設剛好資料量又超過1TB這時候原本的超級電腦就不能使用了，要再買更高級的超級電腦，也就是擴充性不佳
利用多台電腦打造一個運算群集(背後可能是1000台 16核心 48GB的電腦這樣的等級)， 此時要多少運算資源都可以動態的調整 ，
比方說某一個工作要100核心以及2TB的Memory，Spark都可以很快的配置資源，並且這個運算叢集是可以給全公司的人來使用，平均分攤下來的成本較低。
而且這樣的群集的已很容易地增加以及減少裡面的電腦數量。

2. 第二種:
利用多台電腦打造一個運算群集(背後可能是1000台 16核心 48GB的電腦這樣的等級)， 此時要多少運算資源都可以動態的調整 ，
比方說某一個工作要100核心以及2TB的Memory，Spark都可以很快的配置資源，並且這個運算叢集是可以給全公司的人來使用，平均分攤下來的成本較低。
而且這樣的群集的已很容易地增加以及減少裡面的電腦數量。



Spark 優點：
1. 速度快
   Spark 比起Hadoop在運算上快了許多，主要是把資料暫存在記憶體以及把資料處理的部份優化(ex:減少shuffle)
   
2. 容易使用
   Spark提供 Scala,Python,R,Java的API介面，讓開發者可以利用自己擅長的開發語言來開發。主流上是使用Scala, Python這兩種
   
3. 廣泛應用
   可以在Spark上面使用SQL、即時串流(Streaming)、Spark的機器學習套件(MLlib)、圖論(計算圖形中任兩點的最短路徑)的套件
